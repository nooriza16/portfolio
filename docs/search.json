[
  {
    "objectID": "project7.html",
    "href": "project7.html",
    "title": "Identify the most promising locations between 2 potential store investments.",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "project5.html",
    "href": "project5.html",
    "title": "Evaluating a proposed intersection upgrade locations in The City of CHarlotte, North Carolina",
    "section": "",
    "text": "This project is part of a GIS course assessment challenge using Rstudio, focused on helping the City of Charlotte decide where to upgrade intersections after receiving $4.47 million to reduce pedestrian injuries and deaths."
  },
  {
    "objectID": "project5.html#project-summary",
    "href": "project5.html#project-summary",
    "title": "Evaluating a proposed intersection upgrade locations in The City of CHarlotte, North Carolina",
    "section": "Project Summary",
    "text": "Project Summary\n\nPedestrians are considered as a vulnerable road user group\nIn The City of Charlotte, U.S. Department of Transportation federal grants program fund initiatives to prevent roadway deaths and serious injuries, by upgrading and installing equipment to reduce or eliminate pedestrian-involved crashes\nAnalysis is needed to evaluate 22 intersections around The City of Charlotte for upgrades as the project scheduled to receive additional funds in October. Thus, project reviews will be continued through the rest of the year\nThis project will only focus on the areas where the high pedestrian-related crash exists\nChoosing point pattern analysis and spatial autocorrelation to summarise pattern in the crash locations data\nUsing DBSCAN in point pattern analysis will help to create cluster of points\nUsing spatial-autocorrelation (Global Moran & Local Moran’s I) will help to measure how much the presence of one feature is related to the presence of similar features in nearby areas.\nOverlaying the Moran’s Result with proposed intersection upgrade locations"
  },
  {
    "objectID": "project5.html#dataset",
    "href": "project5.html#dataset",
    "title": "Evaluating a proposed intersection upgrade locations in The City of CHarlotte, North Carolina",
    "section": "Dataset",
    "text": "Dataset\n\nMain Dataset : Fatal_or_Serious_Injury_Crashes (shp points) From 2019-2023 I use the pedestrian-related crashes from 2019-2023, with key columns such as: year, month, day, crash type Despite the pandemic occurence in 2020-2022 this analysis includes those data because they still could give values of the occurences and pattern.\nMain Dataset : Safe_Streets_For_All_Grant_Projects (shp points) Dataset of intersection improvement locations Project that contains gisid, location, Description of upgrade projects\nCensus_Commuting_Block_Groups (shp polygon) Contain geo-name with total 555 block groups, this data is chosen because finer level of detailand are specifically used to analyze commuting patterns\n\nAll Datasets projected on NAD83 / North Carolina (ftUS) in US Feet"
  },
  {
    "objectID": "project5.html#question-to-address",
    "href": "project5.html#question-to-address",
    "title": "Evaluating a proposed intersection upgrade locations in The City of CHarlotte, North Carolina",
    "section": "Question to Address",
    "text": "Question to Address\n\nDo Pedestrian-related crashes exhibit a specific pattern across The City of Charlotte between 2019-2023?\nDoes Pedestrian-related crashes vary across The City of Charlotte between 2019-2023?\nAre there any area that has not been covered by the intersection improvement upgrade plans?"
  },
  {
    "objectID": "project5.html#step-1-pre-processing-the-data",
    "href": "project5.html#step-1-pre-processing-the-data",
    "title": "Evaluating a proposed intersection upgrade locations in The City of CHarlotte, North Carolina",
    "section": "Step 1: Pre-Processing the data",
    "text": "Step 1: Pre-Processing the data\n\n#Load libraries\n\nlibrary(here)\n\nhere() starts at E:/portfolio/nooriza.github.io\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ ggplot2   3.5.1     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(readr)\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(here)\nlibrary(sf)\n\nLinking to GEOS 3.12.1, GDAL 3.8.4, PROJ 9.3.1; sf_use_s2() is TRUE\n\nlibrary(sp)\nlibrary(tmap)\n\nBreaking News: tmap 3.x is retiring. Please test v4, e.g. with\nremotes::install_github('r-tmap/tmap')\n\nlibrary(spatstat)\n\nWarning: package 'spatstat' was built under R version 4.4.2\n\n\nLoading required package: spatstat.data\n\n\nWarning: package 'spatstat.data' was built under R version 4.4.2\n\n\nLoading required package: spatstat.univar\n\n\nWarning: package 'spatstat.univar' was built under R version 4.4.2\n\n\nspatstat.univar 3.1-1\nLoading required package: spatstat.geom\n\n\nWarning: package 'spatstat.geom' was built under R version 4.4.2\n\n\nspatstat.geom 3.3-3\nLoading required package: spatstat.random\n\n\nWarning: package 'spatstat.random' was built under R version 4.4.2\n\n\nspatstat.random 3.3-2\nLoading required package: spatstat.explore\n\n\nWarning: package 'spatstat.explore' was built under R version 4.4.2\n\n\nLoading required package: nlme\n\nAttaching package: 'nlme'\n\nThe following object is masked from 'package:dplyr':\n\n    collapse\n\nspatstat.explore 3.3-3\nLoading required package: spatstat.model\n\n\nWarning: package 'spatstat.model' was built under R version 4.4.2\n\n\nLoading required package: rpart\nspatstat.model 3.3-2\nLoading required package: spatstat.linnet\n\n\nWarning: package 'spatstat.linnet' was built under R version 4.4.2\n\n\nspatstat.linnet 3.2-2\n\nspatstat 3.2-1 \nFor an introduction to spatstat, type 'beginner' \n\nlibrary(fpc)\n\nWarning: package 'fpc' was built under R version 4.4.2\n\nlibrary(stringr)\nlibrary(dbscan)\n\nWarning: package 'dbscan' was built under R version 4.4.2\n\n\n\nAttaching package: 'dbscan'\n\nThe following object is masked from 'package:fpc':\n\n    dbscan\n\nThe following object is masked from 'package:stats':\n\n    as.dendrogram\n\nlibrary(ggplot2)\nlibrary(spdep)\n\nWarning: package 'spdep' was built under R version 4.4.2\n\n\nLoading required package: spData\n\n\nWarning: package 'spData' was built under R version 4.4.2\n\n\n\n# Load Data\n\n#The crash dataset\ncrash &lt;- st_read(here::here('Data',\n                          'Fatal_or_Serious_Injury_Crashes.shp'\n                          )) %&gt;% clean_names()\n\nReading layer `Fatal_or_Serious_Injury_Crashes' from data source \n  `E:\\portfolio\\nooriza.github.io\\Data\\Fatal_or_Serious_Injury_Crashes.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 789 features and 20 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 1399484 ymin: 470234.3 xmax: 1511575 ymax: 601314.9\nProjected CRS: NAD83 / North Carolina (ftUS)\n\n#Check data type and columns\nstr(crash)\n\nClasses 'sf' and 'data.frame':  789 obs. of  21 variables:\n $ objectid  : int  5307 5308 5309 5310 5311 5312 5313 5314 5315 5316 ...\n $ crsh_id   : int  969941 1072553 1167265 1227711 960033 1016159 927695 1221628 1126531 1035617 ...\n $ date_val_y: int  2019 2021 2022 2023 2019 2020 2019 2023 2021 2020 ...\n $ date_val_m: int  9 1 7 9 7 2 1 8 11 6 ...\n $ date_val_1: chr  \"September\" \"January\" \"July\" \"September\" ...\n $ date_val_d: int  6 22 25 22 8 17 12 3 23 17 ...\n $ day_of_wee: int  6 6 2 6 2 2 7 5 3 4 ...\n $ day_of_w_1: chr  \"Friday\" \"Friday\" \"Monday\" \"Friday\" ...\n $ milt_time : int  1518 1329 1344 952 204 2300 1439 1507 2113 2021 ...\n $ case_num  : num  2.02e+13 2.02e+13 2.02e+13 2.02e+13 2.02e+13 ...\n $ crsh_type : int  21 23 1 14 3 15 27 1 30 2 ...\n $ crash_type: chr  \"Rear end, slow or stop\" \"Left turn, same roadway\" \"Ran off road right\" \"Pedestrian\" ...\n $ crsh_levl : int  2 2 2 2 2 1 2 2 1 1 ...\n $ primary_ca: int  8 33 33 26 8 NA 11 0 30 6 ...\n $ primary_1 : chr  \"Failure to reduce speed\" \"Unable to determine\" \"Unable to determine\" \"Operated vehicle in erratic, reckless, careless, negligent or aggressive manner\" ...\n $ secondary : chr  NA NA NA NA ...\n $ secondary1: chr  NA NA NA NA ...\n $ latitude  : num  35.3 35.1 35.2 35.3 35.2 ...\n $ longitude : num  -80.8 -80.9 -80.8 -80.7 -80.8 ...\n $ street_id : chr  NA NA NA NA ...\n $ geometry  :sfc_POINT of length 789; first list element:  'XY' num  1451722 566090\n - attr(*, \"sf_column\")= chr \"geometry\"\n - attr(*, \"agr\")= Factor w/ 3 levels \"constant\",\"aggregate\",..: NA NA NA NA NA NA NA NA NA NA ...\n  ..- attr(*, \"names\")= chr [1:20] \"OBJECTID\" \"CRSH_ID\" \"DATE_VAL_Y\" \"DATE_VAL_M\" ...\n\ncolnames(crash)\n\n [1] \"objectid\"   \"crsh_id\"    \"date_val_y\" \"date_val_m\" \"date_val_1\"\n [6] \"date_val_d\" \"day_of_wee\" \"day_of_w_1\" \"milt_time\"  \"case_num\"  \n[11] \"crsh_type\"  \"crash_type\" \"crsh_levl\"  \"primary_ca\" \"primary_1\" \n[16] \"secondary\"  \"secondary1\" \"latitude\"   \"longitude\"  \"street_id\" \n[21] \"geometry\"  \n\n#The intersection improvement locations\nupgrade_points &lt;- st_read(here::here('Data',\n                          'Safe_Streets_For_All_Grant_Projects.shp'\n                          )) %&gt;% clean_names()\n\nReading layer `Safe_Streets_For_All_Grant_Projects' from data source \n  `E:\\portfolio\\nooriza.github.io\\Data\\Safe_Streets_For_All_Grant_Projects.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 22 features and 6 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 1427738 ymin: 483074.8 xmax: 1485674 ymax: 584838.9\nProjected CRS: NAD83 / North Carolina (ftUS)\n\n#The Commuting Block Group Boundary\nAdmin &lt;- st_read(here::here('Data',\n                          'Census_Commuting_Block_Groups.shp'\n                          )) %&gt;% clean_names()\n\nReading layer `Census_Commuting_Block_Groups' from data source \n  `E:\\portfolio\\nooriza.github.io\\Data\\Census_Commuting_Block_Groups.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 555 features and 20 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 1384287 ymin: 460659.4 xmax: 1536942 ymax: 647866.3\nProjected CRS: NAD83 / North Carolina (ftUS)\n\n\n\nFiltering and Transforming Data\n\nWhat I find through the data & will execute are:\n1. There are no NA values accross rows in the key columns of the main dataset\n2. Filtering to pedestrian related crashes only\n3. Make sure that coordinates is appropiate\n\n#Select keycolumns in crash data\nselected_crash &lt;- crash[, c(\"objectid\", \"crsh_id\",\"date_val_y\",\"date_val_1\",\"day_of_w_1\",\"case_num\",   \"crsh_type\",\"crash_type\",\"crsh_levl\", \"primary_1\",\"latitude\",\"longitude\",\"geometry\")]\n\n#Select Pedestrian and make sure points located correctly in North Carolina\nselected_crash1 &lt;- selected_crash %&gt;%\ndplyr::filter(crash_type == \"Pedestrian\") %&gt;%\ndplyr::filter(longitude &lt;0 & latitude &gt; 0) %&gt;% #\ndplyr::distinct(crsh_id, .keep_all = TRUE) #no duplicate of crash recorded \n\n#Reproject All SHPs into projected CRS in metre using EPSG 32119\n\nselected_crash1 &lt;- st_transform(selected_crash1, 32119)\nst_crs(selected_crash1)\n\nCoordinate Reference System:\n  User input: EPSG:32119 \n  wkt:\nPROJCRS[\"NAD83 / North Carolina\",\n    BASEGEOGCRS[\"NAD83\",\n        DATUM[\"North American Datum 1983\",\n            ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4269]],\n    CONVERSION[\"SPCS83 North Carolina zone (meter)\",\n        METHOD[\"Lambert Conic Conformal (2SP)\",\n            ID[\"EPSG\",9802]],\n        PARAMETER[\"Latitude of false origin\",33.75,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8821]],\n        PARAMETER[\"Longitude of false origin\",-79,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8822]],\n        PARAMETER[\"Latitude of 1st standard parallel\",36.1666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8823]],\n        PARAMETER[\"Latitude of 2nd standard parallel\",34.3333333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8824]],\n        PARAMETER[\"Easting at false origin\",609601.22,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8826]],\n        PARAMETER[\"Northing at false origin\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8827]]],\n    CS[Cartesian,2],\n        AXIS[\"easting (X)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"northing (Y)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Engineering survey, topographic mapping.\"],\n        AREA[\"United States (USA) - North Carolina - counties of Alamance; Alexander; Alleghany; Anson; Ashe; Avery; Beaufort; Bertie; Bladen; Brunswick; Buncombe; Burke; Cabarrus; Caldwell; Camden; Carteret; Caswell; Catawba; Chatham; Cherokee; Chowan; Clay; Cleveland; Columbus; Craven; Cumberland; Currituck; Dare; Davidson; Davie; Duplin; Durham; Edgecombe; Forsyth; Franklin; Gaston; Gates; Graham; Granville; Greene; Guilford; Halifax; Harnett; Haywood; Henderson; Hertford; Hoke; Hyde; Iredell; Jackson; Johnston; Jones; Lee; Lenoir; Lincoln; Macon; Madison; Martin; McDowell; Mecklenburg; Mitchell; Montgomery; Moore; Nash; New Hanover; Northampton; Onslow; Orange; Pamlico; Pasquotank; Pender; Perquimans; Person; Pitt; Polk; Randolph; Richmond; Robeson; Rockingham; Rowan; Rutherford; Sampson; Scotland; Stanly; Stokes; Surry; Swain; Transylvania; Tyrrell; Union; Vance; Wake; Warren; Washington; Watauga; Wayne; Wilkes; Wilson; Yadkin; Yancey.\"],\n        BBOX[33.83,-84.33,36.59,-75.38]],\n    ID[\"EPSG\",32119]]\n\n#The intersection improvement locations upgrade\nupgrade_points &lt;- st_transform(upgrade_points, 32119)\n\n#The Commuting Block Group Boundary\nAdmin &lt;- st_transform(Admin, 32119)\n\nselected_Admin &lt;- Admin[, c(\"objectid\", \"geoid10\",\"namelsad10\",\"geoname\")]\n\n#Visually checking the data\ntm_shape(selected_Admin) + tm_polygons(col = NA, alpha = 0.5) +\n  tm_shape(selected_crash1)+ tm_dots(col = \"blue\")  + \n  tm_shape(upgrade_points)+ tm_dots(col = \"green\")"
  },
  {
    "objectID": "project5.html#step-2-data-manipulationprocessing-the-data",
    "href": "project5.html#step-2-data-manipulationprocessing-the-data",
    "title": "Evaluating a proposed intersection upgrade locations in The City of CHarlotte, North Carolina",
    "section": "Step 2 : Data Manipulation/Processing the data",
    "text": "Step 2 : Data Manipulation/Processing the data\n\n#Spatial join between the census commuting block group boundary & crash points plus calculation of crash density\ncrash_sj &lt;- selected_Admin %&gt;%\n  mutate(\n    n = lengths(st_intersects(., selected_crash1))) %&gt;% # Count intersections of pedestrian only\n  mutate(area = st_area(.)) %&gt;%\n  mutate(density = n / area)\n\n#Check to see how it looks like\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\ntm_shape(crash_sj) + tm_polygons(\"density\",\n                                    style = \"jenks\",\n                                    palette = \"Reds\",\n                                    title = \"Crash Density 2019-2023\")\n\n\n\n\nOverall, the density map of pedestrian-related crash occurrences shows that the high occurrences of crashes, whether injury or fatal, are located in the center of the cities but not yield compact clustering rather it scattered outwards into windrose. Hence the map shows a unique pattern, as the ‘dark red (high density)’ follows a slight linear pattern. I suspect this pattern is related to spesific type of road networks, so I will overlay the data using OSM Basemap.\n\n#Overlaying with OSM\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\ntm_shape(crash_sj) + \n  tm_polygons(\"density\",\n              style = \"jenks\",\n              palette = \"Reds\",\n              title = \"Eviction Density 2024\",\n              alpha = 0.5) + #Making the layer transparent is a bad move in map visualization but I will do it here as I want to see the basemap\n  tm_shape(selected_crash1) + tm_dots(col = 'red') +\n  tm_basemap(\"OpenStreetMap\")\n\n\n\n\n\n\nBy overlaying the pedestrian crash, density and basemap the high occurences often found on a Commuting Block near main road such as Central Avenue, North Sharon Amity Road, South Cedar Street etc. From the map can be seen that the Uptown or (city centre) is also the high occurences. The uptown is well known as a most urbanised neighbourhood and well known as a Central Bussiness District [1], so there would be many important services in here that attracts mobility.\nActually the analysis will be deeper if using the network dataset as it helps to identify what kind of road hierarchies the high density will occurs. Besides, adding land use data also will yield a richer analysis as it helps to identify dominant land use this density are mainly located. However it is outside the scope of this short project."
  },
  {
    "objectID": "project5.html#step-3-analysis",
    "href": "project5.html#step-3-analysis",
    "title": "Evaluating a proposed intersection upgrade locations in The City of CHarlotte, North Carolina",
    "section": "Step 3 : Analysis",
    "text": "Step 3 : Analysis\n\nA. Point Pattern Analysis with DBSCAN\nThe purpose is to discover clusters in space. Performing DBSCAN requires:\n\neps\n\nUsing Ripley’s K to get an initial value for further choosing the good epsilon & validating the value using KNN Distribution plot to find a suitable eps value based on the ‘knee’\nb. min pts\nI would choose the median number occurences (n) that spans from 0 to 5 in the commuting block; so the min pts is 3.\n\n#set a window as the borough boundary\nwindow &lt;- as.owin(selected_Admin)\nplot(window)\n\n\n\n#create a sp object\nselected_crash1 &lt;- selected_crash1  %&gt;% #for the points data\n  as(., 'Spatial')\n\n#create a ppp object\nselected_crash1.ppp &lt;- ppp(x=selected_crash1 @coords[,1],\n                          y=selected_crash1 @coords[,2],\n                          window=window)\n\n\nIntuition to find Epsilon with Ripley’s K\n\n#Plotting the RIpley's K graph\nK &lt;- selected_crash1.ppp %&gt;%\n  Kest(., correction=\"border\")%&gt;%\n  plot()\n\n\n\n\nFrom the Ripley’s K plot, we can see that the pedestrian-related crash data is above the theoritical values of poisson distribution(the red line). The plot means that up to a distance of 2500 meters, pedestrian crash cases are very close to each other, showing clustering. Meanwhile, the knee is identified with the largest bulge in the graph starting at around 2500 m, so we use this as the epsilon.\n\n#first extract the points from the spatial points data frame\nselected_crash1Points &lt;- selected_crash1 %&gt;% coordinates(.)%&gt;%as.data.frame()\n\n#now run the dbscan analysis\ndb &lt;- selected_crash1Points %&gt;%\n  fpc::dbscan(.,eps = 2500, MinPts = 3)\n\n#More robust to find eps value based on the ‘knee’ in the KNN plot\nselected_crash1Points%&gt;%\n  dbscan::kNNdistplot(.,k=3)\n\n\n\n#now plot the results\nplot(db, selected_crash1Points, main = \"DBSCAN Output\", frame = F)\nplot(selected_Admin$geometry, add=T)\n\n\n\n\nThe KNN Distribution graph above validates that after calculcating average distance from every 3 points( K neighbours) in a distance around 2500 the largest spike of points sorted by distance happened, this is validating the previous knee value from Ripley’s K: where the value (of distance to neighbours) increases.\nInterpretations of point pattern analysis using DBSCAN: The DBSCAN analysis identifies 5 distinct clusters, with a notable cluster covering the centre (red), northwest (blue), southeast (green), north (purple), northeast(blue). *I hope the colour does not differ if run in another PC, but the direction will mitigate this if that happened.\n\n\n\nB. Spatial Autocorrelation : Moran’s I\na. Performing Global Moran’s I The purpose is to measure similarity between nearby crashes points and provide a value to give a general sense of overall spatial autocorrelation\nb. Performing Local Moran’s I Meanwhile, Local’s Moran I will identify specific locations with similar or dissimilar density values.\n\n#First calculate the centroids of all boroughs\ncentroid_block &lt;- crash_sj%&gt;% #Use the result of spatial joined data with density (polygon)\n  st_centroid()%&gt;%\n  st_geometry()\n\nWarning: st_centroid assumes attributes are constant over geometries\n\n#plot(centroid_block,axes=TRUE)\n\nI create spatial weight matrix using the queen move in this analysis. Queen move suggests that each commuting block polygon is considered a neighbor if it shares either an edge or a corner with another commuting block polygons.\n\n#Make neighbours\ncommunity_nb &lt;- crash_sj %&gt;% #Use the result of spatial joined data with density (polygon)\n  poly2nb(., queen=T)\n\n#Let's see the summary\nsummary(community_nb )\n\nNeighbour list object:\nNumber of regions: 555 \nNumber of nonzero links: 3510 \nPercentage nonzero weights: 1.139518 \nAverage number of links: 6.324324 \nLink number distribution:\n\n  2   3   4   5   6   7   8   9  10  11  12  13 \n  4  16  59 106 132 111  64  40  12   7   2   2 \n4 least connected regions:\n93 170 192 264 with 2 links\n2 most connected regions:\n88 193 with 13 links\n\n\nThe summary tells us that the average number of neighbours is 6.324 which mean that each polygon in the crash_sj (crash occurences point that joined spatially with commuting block) dataset has about 6 neighboring polygons based on the queen contiguity method. Let’s see how the matrix looked across commuting Block in The City of Charlote.\n\n#plot them\nplot(community_nb , st_geometry(centroid_block), col=\"red\")\n#add a map underneath\nplot(crash_sj$geometry, add=T)\n\n\n\n\n\n#Make the weight matrix\ncommunity_nb.lw &lt;- community_nb %&gt;%\n  nb2mat(., style=\"W\") # row standardised : ensuring that each neighborhood's influence is equally distributed among its neighbors\n\nsum(community_nb.lw)\n\n[1] 555\n\n\nThis operation returns 555, which mean that each Commuting Block is linked to other Commuting Blocks and the strength of these connections is evenly distributed across the neighborhoods\n\n#Make weight list for Global Moran's I\ncommunity_nb.lw &lt;- community_nb %&gt;% \n  nb2listw(., style = \"W\")\n\n\nGlobal Moran’s I\nIn Global Moran’s I the value 1 = clustered, 0 = no pattern, -1= disperse\n\nI_globaldensity &lt;- crash_sj %&gt;%\n  pull(density) %&gt;%\n  as.vector() %&gt;%\n  moran.test(.,community_nb.lw )\n\nI_globaldensity\n\n\n    Moran I test under randomisation\n\ndata:  .  \nweights: community_nb.lw    \n\nMoran I statistic standard deviate = 5.6759, p-value = 6.9e-09\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n     0.1332969630     -0.0018050542      0.0005665789 \n\n\nThe interpretation of Global Moran’s I result is: The value 0.133 suggests a weak but positive spatial autocorrelation. This means that there is a slight tendency for similar values to be located near each other, but the clustering is not very strong. To further identify specific areas with slight tendency of sharing similar values I will use Local Moran’s I.\n\n#Perform Local Moran's I\nI_localdensity &lt;- crash_sj %&gt;%\n  pull(density) %&gt;%\n  as.vector() %&gt;%\n  localmoran(.,community_nb.lw ) %&gt;%\n  as.tibble()\n\nWarning: `as.tibble()` was deprecated in tibble 2.0.0.\nℹ Please use `as_tibble()` instead.\nℹ The signature and semantics have changed, see `?as_tibble`.\n\n#From the result we could join the result with the the initial data to make further visualization\n\ncrash_sj_moran &lt;- crash_sj %&gt;%\n  mutate(densityI = as.numeric(I_localdensity$Ii)) %&gt;%\n  mutate(densityIz = as.numeric(I_localdensity$Z.Ii))\n\n\n\n\nStep 4 : Visualization\n\n#set the breaks based on upper bound to ensure that all possible z-scores are included and standard deviation rule\nbreaks1&lt;-c(-1000,-2.58,-1.96,-1.65,1.65,1.96,2.58,1000)\n\n#set the colour\nlibrary(RColorBrewer)\nMoranColours &lt;- brewer.pal(8, \"YlOrRd\")\n\n\nMapping Interactive visualization\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\ntm_shape(crash_sj_moran)+ \n tm_polygons(\n    \"densityIz\", \n    style = \"fixed\", \n    breaks = breaks1, \n    palette = MoranColours, \n    midpoint = NA, \n    alpha = 0.5, #again I choose transparency to see the basemap to identify the land use\n    title = \"Local Moran's I, 2019-2023 Pedestrian-related crash\"\n    ) + \n  tm_basemap(\"OpenStreetMap\")\n\n\n\n\n\n\nTo add more details name, add this to above script\n+tm_text( “geoname”, # Replace with the actual column name for block names size = 0.7, # Adjust text size as needed col = “black”, # Color of the text just = “center”, # Adjust text alignment shadow = TRUE # Add shadow for better visibility, optional )\nInterpretation of Local Moran’s I result :\n\nThe distinctive areas that shares similar high values located in the Uptown Areas [Object Id 4003 and 4397], in North East areas around Commuting Block 5 in Census Track 55.24, Mecklenburg County [Object id 3928,4215]. Honestly, this results are hard to interpret.\nIn terms of finer scale, Census Commuting Block Group does helps to identify a detailed areas, but pinpointing the name of the area is much easier when using the Census tract. However in this analysis, I will interpret the Local Moran’s Result with the help of OSM Basemap. So the high density that shares similar values are:\n\nThe uptown areas/city centre\nSouth East direction from city centre in Evergreen Nature Preserve, which potentially a lot of pedestrian to visit the Park\nNorth East direction from city centre in James Martin Middle School, Governor’s Village Academy, and Julis Chambers High School, which potentially a lot of students commuting from and go to school\n\n\n\n\n\nOverlaying with Upgrade Points From the Government\n\n tm_shape(crash_sj_moran)+ \n tm_polygons(\n    \"densityIz\", \n    style = \"fixed\", \n    breaks = breaks1, \n    palette = MoranColours, \n    midpoint = NA, \n    alpha = 0.5, #again to see the basemap\n    title = \"Local Moran's I, 2019-2023 Pedestrian-related crash\"\n    ) +\n  tm_shape(upgrade_points) + \n  tm_dots(col = \"blue\", size = 0.1)\n\n\n\n\n\n\nFrom the map we can see If the Government want to evaluate the intersection upgrade locations project, then upgrade should includes the 3 areas from the Local’s Moran I which represents the CBD, Parks, and Schools, as these 3 locations have not been included in the upgrade plan. However this result is more appropiate if the result is also verified using road sign distribution data (including sign data such as Accessible Pedestrian Signal (APS), Leading Pedestrian Interval (LPI), Rectangular Rapid Flashing Beacon (RRFB) etc)"
  },
  {
    "objectID": "project5.html#results",
    "href": "project5.html#results",
    "title": "Evaluating a proposed intersection upgrade locations in The City of CHarlotte, North Carolina",
    "section": "Results",
    "text": "Results\n\nThe DBSCAN analysis identifies 5 distinct clusters between 2019-2023, with a notable cluster groups are in the centre, northwest, southeast, north, northeast from the city centre position.\nPedestrian-related crashes show some variation across the City of Charlotte between 2019-2023. The Moran’s I value of 0.133 indicates weak positive spatial autocorrelation, meaning there is a slight tendency for crashes to cluster in certain areas rather than being randomly distributed. While this suggests some localized patterns, the low magnitude of the value implies that the clustering is not very strong.\nThe Local Moran I’s result in the northeast and southeast part of The City of Charlotte aligned with the notion that the further someone gets away from the uptown area, the bigger the dangers for pedestrians [2]. Both of them located far away from the city centre.\nThere are still areas with the highest density of pedestrian-related crashes that have not been covered by the intersection improvement upgrade plans. All these locations are the results identified by Local Moran’s I.\n\nThe strength of this analysis lies in the use of spatial autocorrelation, which identifies specific areas requiring attention. Additionally, the use of an OSM basemap provides contextualization, offering insights into the actual activities occurring in areas with high-neighboring high crash density. I find that the appropriate layer for analyzing pedestrian-related crashes is network data [road polygons], which can calculate crash density based on road segments. While analysis using the total area of the commuting block group is helpful, it tends to generalize the entire block group.\nThe analysis could be more thorough with a dataset on current road sign availability, allowing for more specific recommendations on the types of intersection improvements needed, as well as a land use dataset to better explain potential driving factors based on land use types. Land Use identied in this analysis is based on the Open Street Map which year is not known.\nReferences:\n[1] https://charlotteopenforbusiness.com/meet-charlotte/the-charlotte-edge/real-estate/business-districts/\n[2] Hull & Chandler. (n.d.). Charlotte Among Most Dangerous Cities for Pedestrian Crashes. Retrieved from https://www.hullandchandler.com/blog/charlotte-among-most-dangerous-cities-for-pedestrian-crashes [ accessed online on December 13, 2024]."
  },
  {
    "objectID": "project3.html",
    "href": "project3.html",
    "title": "Investigating London’s Underground Resilience (AM Peak Hour 07.00-10.00)",
    "section": "",
    "text": "This study aims to investigate the resilience of London’s underground system both as an infrastructural network and as a functional mode of transportation using a simplified network. It examines critical stations using various centrality measures, disruption scenarios, and their impact on passengers.\n1) Which station is the most important?\nWell, there are several importance measure can be used here.. To identify important stations based on how many connections a station has we use degree centrality. Turns out Stratford has the most connections ! Well, UCL East is well located then :)\nTo identify the importance of stations by considering its average distance to all other stations, we use closeness centrality. The distance assumed in this study is based on hops between stations (not physical distance), as passengers usually perceive ‘farness’ by the number of stations passed, which is generally more convenient than changing platforms for shorter physical distances (Göransson and Andersson, 2023).\n\nGreen Park, Bank and Museum, and King’s Cross St. Pancras are the top 3 crucial stations based on closeness centrality, indicating that they are best facilitating efficient movement across network.\nMeanwhile, to identify the importance of node by counting how frequently it lies on the shortest paths between other stations we use betweenness centrality.\n\n\n\nThe most crucial stations based on betweenness centrality are Stratford, Bank, and Monument, with Liverpool Street following closely. This highlights their importance as transit hubs. Despite its low degree centrality, Liverpool Street has high betweenness centrality. Newman (2010) described this situation as a low-degree node with high betweenness, acting as a bridge between two network clusters, making Liverpool Street as a hub to and from East London.\n\n\n2) Which station if being disrupted could impact the network’s cohesion?\nThe scenario is conducted by sequentially remove the highest centrality, then recalculate its value and remove the highest again until 10 nodes are removed. In scenario where such happened consecutively in the highest betweenness and degree centrality station, the underground network will become easily fragmented. This indicates that station with high degree and betweenness centrality is important for network cohesion. Additionally, as it is easily disconnecting network into 3 at the earliest removal time, it suggests that it immediately impacts many passengers both arriving at or departing from the disrupted station in 3 different location.\n\nOn the other hand, the most damaging removal occurs with 10 constant removal to nodes with high betweenness centrality, as evidenced by a sharp decline in the largest connected component (LCC = 170) compared to degree and closeness centrality, with LCC 341 and 194 respectively (Figure 5). This suggests that nodes with the highest betweenness centrality are vital for maintaining global connectivity, as their disruption prevents all stations from being reachable from one another.\n\nSo far, we only calculated the network importance without considering the passenger flow. In reality, the importance of London’s underground also related to how many people come in and out from and to a station. When flows are considered, degree and closeness centrality remains unchanged as it measures node proximity, unaffected by flow quantity.\nTop 10 flow weighted closeness centrality\n\nWith flow data, Green Park and Bank & Monument emerge as the most critical stations. Green Park is a key interchange for the Jubilee, Piccadilly, and Victoria lines, and provides access to Heathrow Airport. Its central role makes it prone to overcrowding, as seen on its struggle during the Queen’s Funeral in 2022 (Elvery, 2022).\n3) When flow considered, which station is being impacted the most by the closure scenario? and how many people are affected?\n\n\n\n\n\n\nThe largest flows are found at journey from Waterloo Station to Bank & Monument. If Waterloo station is closed, 67.372 people who use Waterloo as the origin to reach Bank & Monument will be directly affected. Potentially there will be 1.475.019 people have to change their route as they planned to terminate at Waterloo.\n4) Let’s say, Waterloo station is closed which nearest station people should go to?\nIf access to Bank & Monument from Waterloo station is unavailable, Embankment station serves as an alternative to go to Bank & Monument directly with assumption passengers continue their journey with tube. The journey takes approximately 15 minutes, assuming a UK walking speed of 1.2 seconds per meter (Asher et al., 2012), with route as follows:\n\nRemark : The analysis is carried out using NetworkX library in python environment. The GitHub repository is available upon requested (or I will update the script here later), feel free to email me !\nReferences :\nAsher, L. et al. (2012) ‘Most older pedestrians are unable to cross the road in time: a cross-sectional study’, Age and Ageing, 41(5), pp. 690–694. doi: 10.1093/ageing/afs076."
  },
  {
    "objectID": "project1.html",
    "href": "project1.html",
    "title": "Predicting Obesity Prevalence in London: Using Early Life Adversities",
    "section": "",
    "text": "Obesity is major public health concerns and ranked as the fifth leading cause of death worldwide (Safaei et al., 2021). Alarmingly, one in five children is overweight or obese when they start school, and rising to one in three by the end of primary school (House of Commons, 2015). If current trends continue, Britain could become predominantly obese society by 2050 (Butland et al., 2027).\nDespite its prevalence, obesity remains preventable through lifestyle changes, nutritional, and policy interventions (De Lorenzo et al., 2020). Addressing obesity is urgent due to its generational cycle: obese children often become obese adults (Simmonds et al., 2016), and obese adults are more likely to have children with higher birth weights, increasing obesity risk (Office for Health Improvement and Disparities, 2022). Since obesity is caused by acomplex factors (Lingvay et al. (2024), looking at different angles might offer valuable insights.\nThis work aims to explore how early life adversities estimate obesity pravelence, which ground up by findings that repetitive stress could associate with changes in appetite and metabolism (Davis et al., 2014). Pre-teen children (ages 10-11 years) is chosen as they have a higher risk of becoming obese adults compared to preschool children (Ahmad, Ahmad, and Ahmad, 2010). By examining this specific age group, we hope to gain insights into how early life adversity can predict obesity prevalence and identify its key contributing factors."
  },
  {
    "objectID": "project1.html#context",
    "href": "project1.html#context",
    "title": "Predicting Obesity Prevalence in London: Using Early Life Adversities",
    "section": "",
    "text": "Obesity is major public health concerns and ranked as the fifth leading cause of death worldwide (Safaei et al., 2021). Alarmingly, one in five children is overweight or obese when they start school, and rising to one in three by the end of primary school (House of Commons, 2015). If current trends continue, Britain could become predominantly obese society by 2050 (Butland et al., 2027).\nDespite its prevalence, obesity remains preventable through lifestyle changes, nutritional, and policy interventions (De Lorenzo et al., 2020). Addressing obesity is urgent due to its generational cycle: obese children often become obese adults (Simmonds et al., 2016), and obese adults are more likely to have children with higher birth weights, increasing obesity risk (Office for Health Improvement and Disparities, 2022). Since obesity is caused by acomplex factors (Lingvay et al. (2024), looking at different angles might offer valuable insights.\nThis work aims to explore how early life adversities estimate obesity pravelence, which ground up by findings that repetitive stress could associate with changes in appetite and metabolism (Davis et al., 2014). Pre-teen children (ages 10-11 years) is chosen as they have a higher risk of becoming obese adults compared to preschool children (Ahmad, Ahmad, and Ahmad, 2010). By examining this specific age group, we hope to gain insights into how early life adversity can predict obesity prevalence and identify its key contributing factors."
  },
  {
    "objectID": "project1.html#question-to-address",
    "href": "project1.html#question-to-address",
    "title": "Predicting Obesity Prevalence in London: Using Early Life Adversities",
    "section": "Question to address",
    "text": "Question to address\nRQ1: How accurately can early life adversities predict obesity prevalence in pre-teen children in London using various predictive machine learning models?\nRQ2: How do early life adversities contribute to pree-teen’s obesity prevalence in London?"
  },
  {
    "objectID": "project1.html#dataset",
    "href": "project1.html#dataset",
    "title": "Predicting Obesity Prevalence in London: Using Early Life Adversities",
    "section": "Dataset",
    "text": "Dataset"
  },
  {
    "objectID": "project1.html#method",
    "href": "project1.html#method",
    "title": "Predicting Obesity Prevalence in London: Using Early Life Adversities",
    "section": "Method",
    "text": "Method\nAddressing RQ1, linear regression is applied as it provides a simple, fast baseline forpredicting pre-teen obesity. For better accuracy, advanced models like RandomForest (RF) and XGBoost (XGB) are also applied, following Long et al . (2025)’smethodology on UK’s nutritional-based obesity research, as RF and XGB excel withhigh-dimensional, non-linearity, and predictive performance. Meanwhile, to addressRQ2, this study employs Shapley Additive exPlanations (SHAP) to identify featurescontributing to obesity prevalence. The diagram below presents the overall approach followed in this project, showingeach key task in the order they were performed."
  },
  {
    "objectID": "project1.html#result",
    "href": "project1.html#result",
    "title": "Predicting Obesity Prevalence in London: Using Early Life Adversities",
    "section": "Result",
    "text": "Result\n1) Preliminary Result\n\nThe results highlight differences in model performance and generalization: Linear Regression generalizes well, but its predictive power is limited by the linearity assumption, which some features violate, as shown in the correlation matrix While Random Forest and XGBoost fit the training data well, they overfit on the test data. Despite this, they outperform Linear Regression in RMSE, with Random Forest performing better than XGBoost.’\n2) Hyperparameter tuning and Cross Validation result\nAs Linear Regression does not have hyperparameters to tune, hyperparameters tuning will be applied to RF and XGB. GridSearchCV with 5-fold cross-validation is used to optimize performance, aiming to reduce test RMSE and improve R² while maintaining simplicity and efficiency.\n\nRandom Forest\nIn RF, the model’s previous overfitting result suggest that the tree might be too deep, thus max_depth is tested from shallow (3) to moderately deep (12), aiming to capture key patterns without fitting irrelevant details. The min_samples_split and min_samples_leaf is also tuned (up to 15) to prevent overly specific splits. Additionally, n_estimator is optimized, as more trees can improve performance.\n\nThe model reduces overfitting, boosts R² on test data, and sets max_depth to 5 for better generalization. We will examine this hyperparameter individually using validation curve to identify the point at which increasing it leads to overfitting.\n\nWe can see that after max_depth = 5 on validation curve (red), model’s performance on unseen data plateaus, indicating that beyond this point the model likely starting to overfit.\nXGBoost\nIn XGBoost, to optimize model performance, we will vary max_depth to find an optimal depth. The learning_rate will be evaluate to achieve stable adjusted parameter after being iterated. We will also vary n_estimators , as the bigger the number the more stable the model performance. Additionally, reg_lambda will be tested to reduce overfitting by penalize large weights.\n\nAfter tuning, R² increases, overfitting reduced significantly, and the model identifies a max_depth of 4 for good generalization. We will investigate this using a validation curve.\n\nDespite its slightly fluctuative R² score, the pleateu starts to emerge from the max_depth = 4, suggesting that changing the hyperparameter within that range doesn’t significantly improve generalization performance and may increase the risk of overfitting.\n\n3) Comparison of Each Model\n\n\nThe accuracy of machine learning models in predicting pre-teen obesity prevalence in London using early life adversities varies. Random Forest and XGBoost models show higher R² and lower RMSE compared to the baseline linear regression model. Among advanced models, RF has a better performance than XGBoost, with higher R² (0.622 vs. 0.603), with relatively small prediction errors compared to other models listed (0.0334 vs. 0.0342 and 0.0354), and less overfitting before and after hyperparameter tuning.\nAdditionally, Random Forest is faster than XGBoost, even with larger folds. However, even with advanced models like Random Forest, early life adversities can only explain 62.2% of the variance in pre-teen obesity prevalence in London. This leaves 37.8% unexplained. This happened as early life adversity associated with central adiposity in 2 ways, a physiological route (metabolic functioning) and psychosocial route (behaviour) (Davis et al., 2014). Meanwhile, the dataset used in this study does not include physiological aspects, only linking potential proxies for psychosocial, which may account for the unexplained variance.\nTo address RQ2, we will compare feature contributions in the top models, Random Forest and XGBoost, using a SHAP beeswarm plot.\n\n\nThe Random Forest and XGBoost models consistently highlight child poverty, lackof educational qualifications, and mental health index as key contributing factors topre-teen obesity prevalence, underscoring their universal importance. This alignswith Kimbro and Denney ( 2013) which linked obesity with higher poverty level andlow education.\nThe third-ranked mental health index further emphasizes the needfor integrating mental health to obesity interventions. The models diverge in their fourth and fifth rankings: RF places families with 3+children fourth and those working 49+ hours per week fifth, while XGB ranksworking 49+ hours fourth and lone parent unemployment fifth. RF highlightsresource strain in larger households and time strain potentially limiting healthymeal preparation or child supervision, while XGB points to struggles in single-parent households and overtime working (49+ hours exceeds the UK Working TimeDirective).\nThese findings suggests that obesity interventions may benefit from incorporatingat-risk populations with childhood adversities, as well as evaluating supportschemes for work-life balance, unemployed single parents, and large households"
  },
  {
    "objectID": "project1.html#conclusion",
    "href": "project1.html#conclusion",
    "title": "Predicting Obesity Prevalence in London: Using Early Life Adversities",
    "section": "Conclusion",
    "text": "Conclusion\n\nThe Random Forest model outperforms XGBoost and Linear Regression in predicting pre-teen obesity prevalence with higher R² and lower RMSE.\nBoth the Random Forest and XGBoost models consistently identify child poverty, limited educational qualifications, and mental health issues as major contributing factors to pre-teen obesity prevalence in London"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Me",
    "section": "",
    "text": "Hi, I’m Nooriza Maharani, a spatial data analyst with a background in Geography and I’m currently pursuing a Master’s degree at the Centre for Advanced Spatial Analysis (CASA), part of The Bartlett, University College London (UCL) to deepen my GIS skills with the data science and analytic world.\nMy work sits at the intersection of spatial science, data analytic, and urban planning. I’m especially interested in using spatial insights and explore how data can be support decision-making.\n\n\n\n\n\nConnect with me"
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Projects",
    "section": "",
    "text": "List of Projects\n:\n\n\n\n\n\nPredicting Obesity Prevalence in London\nCan childhood hardships predict obesity? Uncovering the link between early life adversity and obesity prevalence by comparing classical regression model, random forest, and XGBoost.\n\n\n\nModelling Rip-Current Rescue using Agent-Based Modelling\nInspired by a rip current tragedy in my hometown, this model visualizes maritime rescue related to rip current incidents from the ground up, using Netlogo.\n\n\n\nInvestigating London’s Underground Resilience\nA study on how to measure the importance of London’s underground network under various scenarios of vulnerabilities using networkX in python.\n\n\n\nSupermarket Investment in the Borough of Havering, London\nWhich location offers the greatest advantage for a new supermarket in the Borough of Havering? A comparison of two proposed sites to find the most beneficial spot using spatial interaction model in python.\n\n\n\nProposing Intersection Upgrade Locations in the City of Charlotte, North Carolina\nThis project is part of a GIS course assessment challenge using Rstudio, focused on helping the City of Charlotte decide where to upgrade intersections after receiving $4.47 million to reduce pedestrian injuries and deaths.\n\n\n\nMapping the distribution of Threatened Group Animals impacted by Forest Loss\nThis project is my Dissertation work, leveraging Google Earth Engine and open source dataset to help Indonesian Government to commit with Kunming Biodiversity Convention, to protect the existing biodiversity."
  },
  {
    "objectID": "project2.html",
    "href": "project2.html",
    "title": "Modelling Rip Current Rescue using Agent Based Modelling",
    "section": "",
    "text": "As a planner, the part of our job is to model a city……which is utterly complex! and then we want to make the ‘computer’ do exactly what we want. How could we build a complex model when we could not even model a simple entity? this is one of main reason draws me to Agent Based Modelling (ABM). ABM teaches me to build a model from scratch, it could turn my abstract or wildest idea into a digital model. Like we could build a game from scratch, that’s fun! As part of the challenge in the assessment during my time in CASA, I create a model inspired by beach hazard incidents in my hometwon in early 2024, caused by a rip current. Here we go…….."
  },
  {
    "objectID": "project2.html#context",
    "href": "project2.html#context",
    "title": "Modelling Rip Current Rescue using Agent Based Modelling",
    "section": "Context",
    "text": "Context\nRip Current is often termed as a beach hazard which make a best swimmer struggle to swim against, as it causes panic and exhaustion lead to drowning (Brighton et al., 2013). The overall purpose of this model is to understand how different response times influence the rescue of bathers’ accident on a beach where rip currents exist. Specifically, we address this question : How is different response times influence the rescue of drowned and lost bathers on a beach with rip currents?\nTo ensure the model is realistic enough to portray rip currents, I use patterns in rip currents’ morphology and current strength. Rip currents is characterised by channels of strong currents (rip neck) flowing from the shore out to sea (rip head) before it weakens (MacMahan, Thornton and Reniers, 2006). The currents drag object away from the shoreline rapidly compared to its surrounding. Rescue operation in this situation is racing with time, as the time passed the survival rates dropped and searching area will bigger while water currents significantly influence bathers’ movement (Grewe and Griva, 2024).\n\nFigure 1 Rip Current Anatomy. Source : University of Hawai’i"
  },
  {
    "objectID": "project2.html#entities-and-state-variables",
    "href": "project2.html#entities-and-state-variables",
    "title": "Modelling Rip Current Rescue using Agent Based Modelling",
    "section": "Entities and State Variables",
    "text": "Entities and State Variables\n\nGlobal variables track drowned, lost, and rescued bathers while initial bathers and rescue time can be modified. Meanwhile, the model spans 66 x 42 grids, each grid represents 20m x 20m. Each tick represents one minute, and the simulation will end after 180 minutes. The strength of rip currents differ depends on the beach topography however in this model we use the recorded current strength of 1 m/s (Yuan et al., 2023) thus bathers moving one patch away equals with 20 m away from the shoreline on each tick."
  },
  {
    "objectID": "project2.html#modelled-patches",
    "href": "project2.html#modelled-patches",
    "title": "Modelling Rip Current Rescue using Agent Based Modelling",
    "section": "Modelled Patches",
    "text": "Modelled Patches\n\nFigure 2 Modelled Patches, each number represent the strength current\n\nFigure 3 Netlogo interface result"
  },
  {
    "objectID": "project2.html#process-overview-and-scheduling",
    "href": "project2.html#process-overview-and-scheduling",
    "title": "Modelling Rip Current Rescue using Agent Based Modelling",
    "section": "Process overview and scheduling",
    "text": "Process overview and scheduling\nThe intuition of the process repeated upon every tick are the bathers will be dragged away from the shoreline with movement depends on the currents’ strength. The movement will cost bathers energy, thus energy will be updated every ticks with detail as follows:\n\nneck area, each patches cost 4 to 7 energy\nhead, each patches costs 4 to 5 energy\nIn non-rip currents area near the shoreline, each patch will cost 2 energy\nIn non-rip currents area away from the shoreline cost 3 energy\n\nAfter that, when the response time set, the ship will navigate its direction depending on target availability in radius of 5 patch either move randomly or to rescue. Upon sensing the target, the ship will move forward faster of 5 patch else it moves 2 patches. After that, when the ship is near enough (radius 3) with the bathers it will rescue, by turning the bathers into pink, and trigger rescued counter. At the same time, in other location bathers which energy depleted will trigger drown counter and bathers hit the right edge will trigger lost counter."
  },
  {
    "objectID": "project2.html#modelling-result",
    "href": "project2.html#modelling-result",
    "title": "Modelling Rip Current Rescue using Agent Based Modelling",
    "section": "Modelling Result",
    "text": "Modelling Result"
  },
  {
    "objectID": "project2.html#demo-video",
    "href": "project2.html#demo-video",
    "title": "Modelling Rip Current Rescue using Agent Based Modelling",
    "section": "Demo Video",
    "text": "Demo Video\n\n\nYour browser does not support the video tag."
  },
  {
    "objectID": "project4.html",
    "href": "project4.html",
    "title": "Identify the most promising locations between 2 potential store investments",
    "section": "",
    "text": "Prompt : Create an analysis to estimate the potential customer for supermarkets in the Borough of Havering under a scenario of increased transport costs and to identify the most promising locations between 2 potential store investments, supermarket E00011326 and E00011710.\n\nDealing with the scenario, I propose using a production-constrained model with an exponential function to estimate potential customer shopping at each store using various mode of transportation. The justification is based on the availability of fixed potential customer data (population data), meanwhile the number of customer shopping at destination remains unknown. With production models, we can identify which shop characteristics attract more customers and estimate the potential market for new store openings, as well as how they affect the market of competitors (Fotheringham, 2001).\nAdditionally, the Production constrained model is commonly used in retail modelling application (Birkin et al., 2015; Newing, Clarke, and Clarke, 2015). This justification aligns with Oshan (2016) suggestion that model selection should depend on data availability and its analysis purpose. To understand how origin characteristics influence interaction patterns, the Production model should be calibrated with baseline data using parameters like γ and β from equation [2]. However, due to the unavailability of baseline data, we use calibrated parameters from commuting flows in London Borough as a proxy, an approach Fotheringham (2001) termed as ‘educated guess’ to address calibration challenges. This introduces a limitation, as the contexts and explanatory variables differ.\n\n\nwhere the cost function reflects a gradual decline in the number of customer shopping as distance increases, accounting for various transportation modes.\n1) Which supermarket is more beneficial to invest?\nIf two new supermarkets are to be invested in the Borough of Havering with the same attractiveness, the more beneficial location would be in the E00011326 Output Areas (OAs) compared to E00011710. With a 2.5% larger population than E00011710 and being more accessible from other OAs in terms of distance, E00011326 can attract 28% more customers, totalling 2.502 potential customer shopping at the store compared to 1.401 from E00011710.\n\nThis occurs because when shopping centres are equally accessible, customers tend to prefer the more attractive one; and when they are equally attractive, shoppers usually favour the more accessible location (Birkin et al., 2015).\n2) Let’s say, there will be a transport increase how does it affect the proposed supermarket location?\nUnder the scenario of a sharp increase in transport costs, investing in E00011326 supermarket remains beneficial as it still generates more customers, increasing 2% from 2.502 to 2.586, compared to E00011710, which saw a 5% decline from 1.401 to 1.261 customer. The top 5 supermarket, undergoing change of customers, including new supermarket of E00011710 is visualized as follows:\n\nIn this scenario, the potential customers shopping at each store are also redistributed with 60% of the 58 stores available in the Borough of Havering witnessing a decrease, 36% of stores gaining more customers, while 2 remain stagnant. The list of top 5 increase and decrease can be seen in table 8 and 9. The shift reflects how behaviour of customers also changes as they become E00011326 sensitive to travel cost/effort. This typically leads customers to prefer closer destinations, as a larger β causes the exponential distance decay function exp (− β𝑑𝑖𝑗 ) to decrease more rapidly with distance, reducing flows to farther destinations and redistributing them toward nearer ones.\n\nThe pattern of consumer’s shopping after doubling transport cost or represented by β change is visualized using plot below. It records the increasing preference to nearby supermarkets before facing a gradual decline to further location.\n\n3) How many people accessing the chosen supermarket within 15 minutes walk?\nAssuming an average walking speed of 1.2 m/s and that each building within a 15-minute walk of the new chosen supermarket (E00011326), in figure 10 , has at least two potential customers, thus the estimated total number of people accessing the supermarket is approximately 3.636. However, this estimate may be lower than the actual number if the area includes apartment buildings with more residents or higher if it contains many office buildings, which may not generate as many supermarket trips as estimated.\n\nRemark : The analysis is using python environment. The GitHub repository is available upon requested (or I will update the script here later), feel free to email me !\nReferences\nBirkin, M. et al. (2015) ‘Chapter 23: Applied spatial interaction modelling in economic geography: an example of the use of models for public sector planning’, in. Available at: https://www.elgaronline.com/display/edcoll/9780857932662/9780857932662.00031.xml (Accessed: 20 April 2025).\nFotheringham, A. S. (2001) ‘Spatial Interaction Models’, in Smelser, N. J. and Baltes, P. B. (eds) International Encyclopedia of the Social & Behavioral Sciences. Oxford: Pergamon, pp. 14794–14800. doi: 10.1016/B0-08-043076-7/02519-5.\nNewing, A., Clarke, G. P. and Clarke, M. (2015) ‘Developing and Applying a Disaggregated Retail Location Model with Extended Retail Demand Estimations’, Geographical Analysis, 47(3), pp. 219–239. doi: 10.1111/gean.12052.\nOshan, T. M. (2016) ‘A primer for working with the Spatial Interaction modeling (SpInt) module in the python spatial analysis library (PySAL)’, REGION, 3(2), p. 11. doi: 10.18335/region.v3i2.175."
  },
  {
    "objectID": "project6.html",
    "href": "project6.html",
    "title": "Mapping the distribution of Threatened Group Animalsimpacted by Forest Loss",
    "section": "",
    "text": "Despite being one of the world’s biodiversity hotspots and facing immense threats, Indonesian Borneo still lacks a comprehensive dataset that identifies where forest loss is occurring and which species’ habitats are being affected. This gap poses a significant challenge to fulfilling the commitments of the Kunming-Montreal Global Biodiversity Framework and advancing broader conservation efforts.\nUsing Google Earth Engine, this research aims to map and quantify the extent to which forest loss affects habitat loss for threatened animal groups, while also assessing the intactness of biodiversity in the remaining habitat.\nAll the dataset used are open source, enabling a reproduceability code for conservation practices or other purposes."
  },
  {
    "objectID": "project6.html#context",
    "href": "project6.html#context",
    "title": "Mapping the distribution of Threatened Group Animalsimpacted by Forest Loss",
    "section": "",
    "text": "Despite being one of the world’s biodiversity hotspots and facing immense threats, Indonesian Borneo still lacks a comprehensive dataset that identifies where forest loss is occurring and which species’ habitats are being affected. This gap poses a significant challenge to fulfilling the commitments of the Kunming-Montreal Global Biodiversity Framework and advancing broader conservation efforts.\nUsing Google Earth Engine, this research aims to map and quantify the extent to which forest loss affects habitat loss for threatened animal groups, while also assessing the intactness of biodiversity in the remaining habitat.\nAll the dataset used are open source, enabling a reproduceability code for conservation practices or other purposes."
  },
  {
    "objectID": "project6.html#dataset",
    "href": "project6.html#dataset",
    "title": "Mapping the distribution of Threatened Group Animalsimpacted by Forest Loss",
    "section": "Dataset",
    "text": "Dataset"
  },
  {
    "objectID": "project6.html#method",
    "href": "project6.html#method",
    "title": "Mapping the distribution of Threatened Group Animalsimpacted by Forest Loss",
    "section": "Method",
    "text": "Method"
  },
  {
    "objectID": "project6.html#result",
    "href": "project6.html#result",
    "title": "Mapping the distribution of Threatened Group Animalsimpacted by Forest Loss",
    "section": "Result",
    "text": "Result\n\n\nLink to GEE platform:\nhttps://code.earthengine.google.com/8b463b50ceaa044a38e15edf600495eb\nhttps://code.earthengine.google.com/8b463b50ceaa044a38e15edf600495eb&lt;/a&gt;&lt;a href=\"https://your-app-url.earthengine.app/view/myapp%22\"&gt;\"&lt;/a&gt;width=\"100%\" height=\"600\" style=\"border:none;\"&gt;&lt;/p&gt;"
  }
]